import numpy as np 
import pandas as pd 
import seaborn as sns  
import matplotlib.pyplot as plt  
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score


df = pd.read_csv("/home/a/notebook/machinelearning/malwareDetection/dataset/dataset_malwares.csv")
df.head()

df.info()

# The dataset provided has already been cleaned, consistent data types and no missing data

dropped_df = df.drop(['Name', 'Machine', 'TimeDateStamp', 'Malware'], axis = 1)

X = dropped_df
y = df['Malware']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, 
                                                    random_state = 42, 
                                                    stratify = y)


print("Number of used features: ", X_train.shape[1])

clf_gini = RandomForestClassifier(
	n_estimators = 100,
	random_state = 0,
	oob_score = True,
	max_depth = 16)
clf_gini.fit(X_train, y_train)

y_pred = clf_gini.predict(X_test)
gini_accuracy = accuracy_score(y_test, y_pred)

clf_entropy = RandomForestClassifier(
    criterion = 'entropy',
    n_estimators = 100,
    random_state = 0,
    oob_score = True,
    max_depth = 16)
clf_entropy.fit(X_train, y_train)

entropy_pred = clf_entropy.predict(X_test)
entropy_accuracy = accuracy_score(y_test, entropy_pred)

clf_log_loss = RandomForestClassifier(
    criterion = 'log_loss',
    n_estimators = 100,
    random_state = 0,
    oob_score = True,
    max_depth = 16
)
clf_log_loss.fit(X_train, y_train)

log_loss_pred = clf_log_loss.predict(X_test)
log_loss_accuracy = accuracy_score(y_test, log_loss_pred)

#A/B Testing
print(f"Accuracy with Gini Criterion: {gini_accuracy:.10f}")
print(f"Accuracy with Entropy Criterion: {entropy_accuracy:.10f}")
print(f"Accuracy with Log Loss Criterion: {log_loss_accuracy:.10f}")


if gini_accuracy > entropy_accuracy:
    print("The RF model with Gini Criterion performs better")
elif entropy_accuracy > log_loss_accuracy:
    print("The RF model with Entropy Criterion is better")
else:
    print("The RF model with Log Loss Criterion performs better")


print(classification_report(y_test, entropy_pred, target_names = ['Not Malware', 'Malware']))

matrix = confusion_matrix(y_test, entropy_pred)
ax = sns.heatmap(matrix,
	annot = True,
	fmt = "d",
	cmap = 'rocket_r',
	cbar = False,
	xticklabels = ['Not Malware', 'Malware'],
	yticklabels = ['Not Malware', 'Malware'])
ax.set_xlabel('Predicted Labels')
ax.set_ylabel('True Labels')

# the code below was a first attempt at feature importances, vizualisation was ugly. 
'''###
importance = clf_entropy.feature_importances_
importance_dict = dict(zip(dropped_df.columns.values,
	importance))
sorted_importance = dict(sorted(importance_dict.items(),
  key=lambda x: x[1],
  reverse = True))

plt.figure(figsize = (40, 20))
sns.barplot(x=list(sorted_importance.values()), 
	y = list(sorted_importance.keys()))
plt.xlabel('Importance Value')
plt.ylabel('Feature Name')
plt.title('Feature IMportance in Random Forest Classifier')
plt.show()
###'''

# Below is a better vizualisation of the top 20 important features according to the RF

importance = clf_entropy.feature_importances_
top_20 = np.argsort(importance)[-20:]
feature_names = X_train.columns
top_20_feature_names = feature_names[top_20]
top_20_feature_importance = importance[top_20]

plt.figure(figsize = (20, 10))
plt.barh(range(len(top_20_feature_importance)), 
         top_20_feature_importance, 
         align = 'center')
plt.yticks(range(len(top_20_feature_importance)),
           top_20_feature_names)
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
plt.title('Top 20 Feature Importance')
plt.show()
