from sklearn.model_selection import train_test_split,  cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score 

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
df = pd.read_csv("/home/a/notebook/machinelearning/malwareDetection/dataset/dataset_malwares.csv")
df.head()

df.info()

dropped_df = df.drop(['Name', 'Machine', 'TimeDateStamp', 'Malware'], axis = 1)

X = dropped_df
y = df['Malware']

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    random_state = 42, 
                                                    test_size = 0.2,
                                                    stratify = y)

print("Number of used feature: ", X_train.shape[1])

# Random Forest Entropy Model
clf_entropy = RandomForestClassifier(
    n_estimators = 100,
    criterion = 'entropy',
    random_state = 0,
    oob_score = True,
    max_depth = 16
)
clf_entropy.fit(X_train, y_train)

RF_pred = clf_entropy.predict(X_test)
RF_accuracy = accuracy_score(y_test, RF_pred)
print(f"RF Accuracy: {RF_accuracy:.10f}")

# First Gradient Boost model
gradient_boost = GradientBoostingClassifier(
    loss = 'log_loss',
    criterion = 'friedman_mse',
    learning_rate = 0.1,
    n_estimators = 100,
    random_state = 0,
    max_depth = 16
)
gradient_boost.fit(X_train, y_train)

GB_pred = gradient_boost.predict(X_test)
GB_accuracy = accuracy_score(y_test, GB_pred)
print(f"GB accuracy: {GB_accuracy:.10f}")

# Second Gradient Boost Model
gradient_boost2 = GradientBoostingClassifier(
    loss = 'exponential',
    criterion = 'squared_error',
    learning_rate = 0.1,
    n_estimators = 200,
    random_state = 42,
    max_depth = 3
)
gradient_boost2.fit(X_train, y_train)

GB2_pred = gradient_boost.predict(X_test)
GB2_accuracy = accuracy_score(y_test, GB2_pred)
print(f"GB2 accuracy: {GB2_accuracy:.10f}")

# Bagging Classification
bagging = BaggingClassifier(
    n_estimators = 10,
    max_samples = 1.0,
    random_state = 0,
    oob_score = False,
    max_features = 1.0
)
bagging.fit(X_train, y_train)

bagging_pred = bagging.predict(X_test)
bagging_accuracy = accuracy_score(y_test, bagging_pred)
print(f"Bagging Accuracy: {bagging_accuracy:.10f}")

# Second Bagging Classification, Parameter Tuning
bagging2 = BaggingClassifier(
    n_estimators = 20,
    max_samples = 1.0,
    random_state = 42,
    bootstrap = True,
    bootstrap_features = True,
    oob_score = True, # Only allowed if bootstrap is set to True
    max_features = 1.0,
)
bagging2.fit(X_train, y_train)

bagging2_pred = bagging2.predict(X_test)
bagging2_accuracy = accuracy_score(y_test, bagging2_pred)
print(f"Second Bagging Accuracy: {bagging2_accuracy:.10f}")

# AdaBoost Classifier
adaboost = AdaBoostClassifier(
    n_estimators = 50,
    learning_rate = 1.0,
    algorithm = 'SAMME.R',
    random_state = 0,
    base_estimator = None
)
adaboost.fit(X_train, y_train)
adaboost_pred = adaboost.predict(X_test)
adaboost_accuracy = accuracy_score(y_test, adaboost_pred)
print(f"AdaBoost Accuracy: {adaboost_accuracy:.10f}")

# AdaBoost Classifier 2
adaboost2 = AdaBoostClassifier(
    n_estimators = 100,
    learning_rate = 1.0,
    algorithm = 'SAMME',
    random_state = 42,
    base_estimator = None
)
adaboost2.fit(X_train, y_train)
adaboost_pred2 = adaboost2.predict(X_test)
adaboost_accuracy2 = accuracy_score(y_test, adaboost_pred2)
print(f"AdaBoost 2 Accuracy: {adaboost_accuracy2:.10f}")

# Original cross valuation score when comparing just the 2 gradient boost models
# Ultimately comment it out as I want to do Cross val with all the models
# Over engineered

'''###
cv_score_model_GB1 = cross_val_score(gradient_boost, X, y, cv = 5)
cv_score_model_GB2 = cross_val_score(gradient_boost2, X, y, cv = 5)

print("Cross Validation Score for Gradient Boost 1: ", cv_score_model_GB1)
print("Average Cross Validation score for Gradient Boost 1: ", cv_score_model_GB1.mean())

print("Cross Validation Score for Gradient Boost 2: ", cv_score_model_GB2)
print("Average Cross Validation score for Gradient Boost 2: ", cv_score_model_GB2.mean())

if cv_score_model_GB1.mean() > cv_score_model_GB2.mean():
    print("Gradient Boost 1 performs better")
elif cv_score_model_GB2.mean() > cv_score_model_GB1.mean():
    print("Gradient Boost 2 performs better")
else:
    print("They Are The Same")
###'''

# More Simple Cross Val score for all models

models = [
    ("Random Forest", clf_entropy),
    ("Gradient Boost", gradient_boost),
    ("Gradient Boost 2", gradient_boost2),
    ("Bagging", bagging),
    ("Bagging 2", bagging2),
    ("Ada Boost", adaboost),
    ("Ada Boost 2", adaboost2)
]

for name, model in models:
    scores = cross_val_score(model, X, y, cv = 5)
    print(f"{name}: Accuracy: {scores.mean():.6f} (+/- {scores.std() * 2:.6f})")

